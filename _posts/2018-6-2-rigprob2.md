---
layout: post
title: Rigorous Probability Ch. 2 Notes
---

This summer I've been reading through [A First Look at Rigorous Probability][1] by Jeffrey S. Rosenthal. So far (27 pages in), it's proving to be a pretty clearly written textbook that explains concepts well. I will try to post notes for each chapter, as it helps me understand the material better. But we will see if I can keep this up :O

## Basic Definitions

In a typical undergraduate course in probability ([such as this][2]), we are introduced to the concepts of sample space, events, and a probability measure. For the most part, this is adequate - we are usually operating in discrete probability spaces (i.e. Poisson distribution), or we have a "nice" continuous distribution with well defined probabilities $$P(X\in\text{dx})$$.

However, if we dig deeper, we find that this understanding of probability *breaks down*. Rosenthal gives the motivation in Ch. 1 of trying to construct a uniform probability measure on $$[0, 1]$$ that 1) has the natural property that $$P([a, b]) = b-a$$, 2) is countably additive, 3) has the property that $$P(A \oplus r) = P(A)$$, where $$A\oplus r$$ denotes the right shift of A. In fact, there exists no probability measure $$P(A)$$ defined on *all* subsets of $$[0,1]$$ that satisfies those 3 properties!

Hence, we must restrict our probability measure to some subsets of our sample space, where these properties are defined. This is how we do so:

We define a probability triple: $$(\Omega, \mathcal{F}, P)$$:

* $$\Omega$$ is our sample space.
* $$\mathcal{F}$$ is a $$\sigma$$-algebra: a collection of subsets of $$\Omega$$ that:
	* Contain $$\Omega$$ and $$\emptyset$$.
	* Closed under complements: if $$A\in\mathcal{F}$$, $$A^C\in\mathcal{F}$$.
	* Closed under **countable** unions and intersections: if $$A_1, A_2, ... \in \mathcal{F}$$, $$\bigcup_{i=1}^\infty A_i \in\mathcal{F}$$ and $$\bigcap_{i=1}^\infty A_i \in\mathcal{F}$$.
* $$P$$ is our probability measure: to each element $$A\in\mathcal{F}$$ we assign $$P(A) \in[0,1]$$. $$P(\emptyset)=0$$ and $$P(\Omega) = 1$$. $$P$$ is *countably additive*.

We also have some properties like: monotonicity, PIE, and countable subadditivity.

Discrete probability spaces present no difficulties, and indeed we can take our $$\mathcal{F}$$ to be the power set $$2^{\Omega}$$ and everything is nice.

We also have this concept of a semialgebra $$\mathcal{J}$$: a collection of subsets of $$\Omega$$ that contains $$\emptyset$$, $$\Omega$$, is closed under *finite intersection*, and has the property that the complement of any element of $$\mathcal{J}$$ is equal to a finite disjoint union of elements of $$\mathcal{J}$$.

## Extension Theorem
We are motivated by the question posed in Ch. 1: how can we construct a valid probability measure on $$[0,1]$$? An important theorem allows us to do this: we can first construct the probability measure on a semialgebra $$\mathcal{J}$$ (which is usually "simple" subsets") and then *extend* it to a more complicated $$\sigma$$-algebra. Here I state the theorem. The proof can be found in the book.

### Theorem 2.3.1
Let $$\mathcal{J}$$ be a semialgebra of subsets of $$\Omega$$. Let $$P:\mathcal{J} \to [0,1]$$ with $$P(\emptyset)=0$$ and $$P(\Omega) = 1$$.

We need two properties:

* Finite superadditivity: If disjoint $$A_1,...,A_k\in\mathcal{J}$$, $$\bigcup_{i=1}^k A_i\in\mathcal{J}$$:

	$$ P(\bigcup_{i=1}^k A_i) \ge \sum_{i=1}^k P(A_i)$$
	
* Countable monotonicity: If $$A, A_1,A_2...,\in\mathcal{J}$$, $$A\subseteq \bigcup_n A_n$$, then: 

$$P(A) \le \sum_n P(A_n)$$

Then:

* There exists a $$\sigma$$-algebra $$\mathcal{M} \supseteq \mathcal{J}$$.
* Probability measure $$P^*$$ on $$\mathcal{M}$$ that is *countably additive*.
* $$P^*(A) = P(A)$$ for all $$A\in\mathcal{J}$$ (They agree!)

Our $$P^*$$ is defined as an outer measure using $$P(A)$$ (see book). Also, our $$\mathcal{M}$$ is defined as:

$$\mathcal{M} = \{ A\subseteq \Omega: P^*(A \cap E) + P^*(A^C\cup E) = P^*(E) \text{ } \forall E\subseteq\Omega\}$$

Sometimes that equal sign is written as a $$\le$$, a fact that confused me for a long time. But basically, by subadditivity we always have $$\ge$$, so when we are proving things (like whether some object is in $$\mathcal{M}$$, it suffices to show $$\le$$.

## Then?
We can use this theorem to construct the Uniform[0,1] distribution, defining the *Borel $$\sigma$$-algebra* and the *Lebesgue measure* on the way!

In addition, there are other corollarries to the extension theorem, because the **countable monotinicity** property is sometimes hard to verify.

We can also construct a "coin tossing measure" for when we want to toss an ~infinite~ number of coins. (This can be interpreted by the Lebesgue measure we defined when we were constructing the Uniform[0,1] measure.

Lastly, we have the **product measure**: if we have $$(\Omega_1, \mathcal{F}_1, P_1)$$ and $$(\Omega_2, \mathcal{F}_2, P_2)$$, we can define $$P$$ on $$\Omega_1 \times \Omega_2$$.

Set $$\mathcal{J} = {A \times B; A\in \mathcal{F}_1, B \in \mathcal{F}_2}$$. For all $$A\times B \in \mathcal{J}$$:

$$P(A\times B) = P_1(A) P_2(B).$$

For example, we can measure "area" on $$\Omega = [0,1] \times [0,1]$$ in this way.

Cool stuff.




[1]:https://www.amazon.com/First-Look-Rigorous-Probability-Theory/dp/9812703713
[2]:http://www.princeton.edu/~rvan/ORF309.pdf